## AtomicHack

Так как в датасете 1400 значения в основном большие, мы сочли необходимым добавить к нему датасет 35000, но лишь треть от него ввиду его огромного размера, для этого мы взяли признак IC50 и SMILE, объедили датасеты, распарсили SMILE через дескриптор и после этого начали подбирать регрессионные модели для этого датасета, предварительно из распарсленного SMILE мы выделили 10 самых коррелируемых с IC50 признаков, по которым в будущем планировали строить регрессию
Остановили мы на линейной мультирегрессии, однако перед этим убрали неадекватные выбросы из датасета, поскольку для IC50 чем меньше значение, тем лучше, то мы убрали верхний 95%-квантиль, чтобы избавиться от аномалий, по итогу получилась модель линейной регрессии с точностью 90% и MSE около 300
Затем мы решили улучшить решение. Поскольку второй датасет относится к разным штаммам гриппа, то пришла идея разбить на кластеры через кластеризацию методом kmeans++, предполагалось что так мы получим по кластерам лекарства с примерно схожими свойствами, и возможно, для сходных штаммов. Значит и внутри каждого кластера можно будет обучить модель, полученную на предыдущем этапе с более высокой точностью, однако эта гипотеза не подтвердилась и точность модели осталась примерно в том же диапазоне, а иногда и становилась хуже. Причина вероятно в том, что гипотеза о разбиении по штаммам неверна из-за того, что мы не знаем какие конкретно параметры важны для разбиения по штаммам, и более того, кластеризация ни в коем разе не гарантирует разбиение по нужным нам свойствам
В процессе обучения, возникали проблемы с тем, что модель переобучалась на большом датасете, из-за чего точность отказывалась неестественно высокой (MSE < 1), а на малом датасете крайне низкой (MSE > 1), поэтому было принято решение соединить два датасета и точность примерно выровнялась.
Перебрав разные модели, мы решили остановиться на ансамблях, а точнее на градиентном бустинге. Этот метод показывал хорошие метрики, относительно остальных моделей. Также мы решили дополнительно улучшить этот метод, перебрав его параметры до оптимальных. Градиентный бустинг по эффективности сопоставим с нейронными сетями, поэтому логично, что он имеет хорошую обучаемость на данном датасете.
